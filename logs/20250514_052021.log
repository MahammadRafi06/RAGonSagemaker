2025-05-14 05:20:22,262, ragwithsagemaker, common.py, 19, read_yaml, INFO, config/config.yaml file loaded sucessfully
2025-05-14 05:20:22,263, ragwithsagemaker, common.py, 19, read_yaml, INFO, params.yaml file loaded sucessfully
2025-05-14 05:20:22,263, ragwithsagemaker, common.py, 19, read_yaml, INFO, schema.yaml file loaded sucessfully
2025-05-14 05:20:22,263, ragwithsagemaker, common.py, 32, create_dir, INFO, Directory model created
2025-05-14 05:20:22,263, ragwithsagemaker, textgenerationmodel.py, 16, __init__, INFO, config received SagemakerSessionConfig(bucket='mlflowtrackingrafi', default_bucket_prefix='model', role='arn:aws:iam::703671901662:role/service-role/AmazonSageMaker-ExecutionRole-20250211T170358') TextgenartionConfig(servingproperties=ConfigBox({'engine': 'MPI', 'option.model_id': 'tiiuae/falcon-7b-instruct', 'option.trust_remote_code': 'true', 'option.tensor_parallel_degree': 1, 'option.paged_attention': 'true', 'option.max_rolling_batch_size': 64, 'option.rolling_batch': 'lmi-dist', 'option.max_rolling_batch_prefill_tokens': 1560}), model=ConfigBox({'model_name': 'falcon-7b-instruct', 'properties_file': 'tiiuae'}), image=ConfigBox({'framework': 'djl-deepspeed', 'version': '0.23.0'}), instance_type='ml.g5.2xlarge', model_folder='model', base_name_endpoint='lmi-model-falcon-7b', s3_code_prefix='large-model-lmi/code')
2025-05-14 05:20:22,408, ragwithsagemaker, textgenerationmodel.py, 43, creat_and_deploy_model, INFO, Propteries file written to location
2025-05-14 05:20:22,414, ragwithsagemaker, textgenerationmodel.py, 47, creat_and_deploy_model, INFO, Tar file generated
2025-05-14 05:20:22,505, ragwithsagemaker, textgenerationmodel.py, 50, creat_and_deploy_model, INFO, image uri is : 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118
2025-05-14 05:20:22,951, ragwithsagemaker, textgenerationmodel.py, 57, creat_and_deploy_model, INFO, code artificats pushed to s3
2025-05-14 05:20:22,951, ragwithsagemaker, textgenerationmodel.py, 59, creat_and_deploy_model, INFO, model name is: lmi-model-falcon-7b-2025-05-14-12-20-22-951
2025-05-14 05:20:22,951, ragwithsagemaker, textgenerationmodel.py, 68, creat_and_deploy_model, INFO, model generated
2025-05-14 05:20:25,001, ragwithsagemaker, embeddingmodel.py, 17, __init__, INFO, config received SagemakerSessionConfig(bucket='mlflowtrackingrafi', default_bucket_prefix='model', role='arn:aws:iam::703671901662:role/service-role/AmazonSageMaker-ExecutionRole-20250211T170358') EmbeddingsConfig(instance_type='ml.g5.4xlarge', model_version='*', model_id='huggingface-textembedding-gpt-j-6b-fp16', model_scope='inference', image_scope='inference', env={'"SAGEMAKER_MODEL_SERVER_WORKERS"': '1', '"TS_DEFAULT_WORKERS_PER_MODEL"': '1'}, role='arn:aws:iam::703671901662:role/service-role/AmazonSageMaker-ExecutionRole-20250211T170358')
2025-05-14 05:20:25,001, ragwithsagemaker, embeddingmodel.py, 25, __init__, INFO, config applied ml.g5.4xlarge * huggingface-textembedding-gpt-j-6b-fp16 
                    inferenceinference{'"SAGEMAKER_MODEL_SERVER_WORKERS"': '1', '"TS_DEFAULT_WORKERS_PER_MODEL"': '1'}arn:aws:iam::703671901662:role/service-role/AmazonSageMaker-ExecutionRole-20250211T170358
2025-05-14 05:20:26,740, ragwithsagemaker, embeddingmodel.py, 38, deploy_embedding_model, INFO, model_uri is : s3://jumpstart-cache-prod-us-east-1/huggingface-infer/prepack/v1.0.1/infer-prepack-huggingface-textembedding-gpt-j-6b-fp16.tar.gz
2025-05-14 05:20:26,740, ragwithsagemaker, embeddingmodel.py, 40, deploy_embedding_model, INFO, end point name is : huggingface-textembedding-gpt-j-6b-fp16-2025-05-14-12-20-26-740
2025-05-14 05:20:26,754, ragwithsagemaker, embeddingmodel.py, 50, deploy_embedding_model, INFO, Deployment Image is : 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-gpu-py38
2025-05-14 05:20:26,754, ragwithsagemaker, embeddingmodel.py, 60, deploy_embedding_model, INFO, Model is : <sagemaker.model.Model object at 0x7f8df44902b0>
